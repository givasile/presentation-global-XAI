\begin{frame}
  \frametitle{Example}
  \begin{onlyenv}<1>
    Consider the following mapping $x \rightarrow y$
    \begin{center}
      \scalebox{0.5}{
        \input{figures/chapter2/ovf_1_reality.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<2>
    Process unknown \(\rightarrow\) we only have samples
    \begin{center}
      \scalebox{0.5}{
        \input{figures/chapter2/ovf_2_sampling.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<3>
    Our goal is to model the process using the available samples (regression)
    \vspace{1cm}\\
  \end{onlyenv}
  \begin{onlyenv}<4>
    Linear model \(\rightarrow\) Underfiting!
    \begin{equation*}
      y = w_1\cdot x + w_0
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/chapter2/ovf_3_linear.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<5>
    2$^{nd}$ degree polynomial \(\rightarrow\) Decent Fit!
    \begin{equation*}
      y = w_2\cdot x^2 + w_1\cdot x + w_0
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/chapter2/ovf_4_quadratic.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<6>
    3$^{rd}$ degree polynomial \(\rightarrow\) Good Fit!
    \begin{equation*}
      y = w_3\cdot x^3 + w_2\cdot x^2 + w_1\cdot x + w_0
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/chapter2/ovf_5_3d.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<7>
    9$^{th}$ degree polynomial \(\rightarrow\) Overfitting!
    \begin{equation*}
      y = \sum_{i=0}^{9}w_i\cdot x^{i}
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/chapter2/ovf_6_9d.pgf}
      }
    \end{center}
  \end{onlyenv}
\end{frame}

\begin{frame}
  \frametitle{Feature Effect Methods}

  \begin{itemize}
  \item Model behavior is \emph{explained} by the shape of the function
  \item Overfitting, Underfitting are easily diagnosed
  \item If high-dimensional input \(\xb \in \mathbb{R}^D\)?
    \begin{itemize}
    \item Tabular data; tens or hundreds of features
    \item Images and signals; several thousands of input dimensions
    \end{itemize}
  \item \(x_s \rightarrow \) feature of interest
  \item \(\Vx_c \rightarrow\) other features
  \item How do we isolate the effect of \(x_s\)?
  \end{itemize}
\end{frame}



\begin{frame}
  \frametitle{Running Example: Bike Sharing Problem}

  \begin{itemize}
  \item Predict Bike rentals per hour in California
  \item We have 11 features
    \begin{itemize}
    \item e.g., month, hour, temperature, humidity, windspeed
    \end{itemize}
  \item We fit a ML model \(y = \hat{f}(\xb)\)
  \end{itemize}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  How each feature affects the output?
\end{frame}


\subsection{PDP}
\begin{frame}
 \frametitle{Partial Dependence Plots (PDP)}
 \begin{onlyenv}<1>
   \begin{itemize}
   \item Proposed by J. Friedman on 2001\footnote{\href{https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full}{Friedman et. al (2001)}} and is the marginal \emph{effect} of a feature to the
     model output:
     \begin{equation*}
       f_s(x_s) = E_{X_c}\left[\hat{f}(x_s, X_c)\right]
     \end{equation*}
   \item Computation:
     \begin{equation*}
       \hat{f}_s(x_s) = \frac{1}{n}\sum\limits_{i=1}^{n}\hat{f}(x_s, \Vx_c^{(i)})
     \end{equation*}
   \end{itemize}
 \end{onlyenv}
 \begin{onlyenv}<2>
   \emph{Bike sharing Dataset:}
   \begin{figure}
     \includegraphics[width=\textwidth]{pdp-bike-1}
     \caption{\footnotesize C. Molnar, IML book}
   \end{figure}
 \end{onlyenv}
\end{frame}

\begin{frame}
 \frametitle{Issues with PDPs}
 \begin{onlyenv}<1>
   \begin{itemize}
   \item The marginal distribution ignores correlated features!
   \item To compute the effect of temperature $=33$ degrees it will (also) use an instance
       with month = January
   \end{itemize}
   \begin{figure}
     \includegraphics[width=.6\textwidth]{aleplot-motivation1-1}
     \caption{\footnotesize C. Molnar, IML book}
   \end{figure}
 \end{onlyenv}
\end{frame}

\subsection{ALE}
\begin{frame}
 \frametitle{Accumulated Local Effects (ALE)\footnote{D. Apley and
   J. Zhu. ``Visualizing the effects of predictor variables in black box
   supervised learning models.'' Journal of the Royal Statistical Society:
   Series B (Statistical Methodology) 82.4 (2020): 1059-1086.}}

 \begin{itemize}
 \item Resolves problems that result from the feature correlation by computing
   differences over a (small) window
 \item Definition: \(f(x_s) = \int_{x_{min}}^{x_s}\mathbb{E}_{\Vx_c|z}[ \frac{\partial f}{\partial x_s}(z, \Vx_c)] \partial z\)
 \end{itemize}
\end{frame}

\begin{frame}
 \frametitle{ALE approximation}
 Approximation: \(f(x_s) = \sum\limits_{k=1}^{k_x}
 \underbrace{\frac{1}{|\mathcal{S}_k|} \sum_{i:\Vx^i \in \mathcal{S}_k}
   \underbrace{[f(z_k, \Vx^i_c) - f(z_{k-1}, \Vx^i_c)]}_{\text{point
       effect}}}_{\text{bin effect}} \)

 \begin{figure}[ht]
   \centering
   \includegraphics[width=0.65\textwidth]{ale_bins_iml.png}
   \caption{\footnotesize C. Molnar, IML book}
 \end{figure}
\end{frame}

\begin{frame}
 \frametitle{ALE plots - examples}
 \begin{figure}
   \includegraphics[width=1\textwidth]{ale-bike-1}
   \caption{\footnotesize C. Molnar, IML book}
 \end{figure}
\end{frame}

\subsection{DALE}
\begin{frame}
  \frametitle{Our work}

  \begin{itemize}
  \item Differential Accumumulated Local Effects (DALE)
    \begin{itemize}
    \item Asian Conference in Machine Learning (ACML 2022)
    \item Work done with: Christos Diou, Theodore Dalamagas
    \end{itemize}
  \item More efficient and accurate extension of ALE
  \item Works only with differential models (like Neural Networks)
  \item \href{https://arxiv.org/abs/2210.04542}{https://arxiv.org/abs/2210.04542}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Our proposal: Differential ALE}
    \[f(x_s) = \Delta x \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|} \sum_{i:\xb^i \in \mathcal{S}_k} \underbrace{[\frac{\partial f}{\partial x_s}(x_s^i, \bm{x^i_c})]}_{\text{\alert{point effect}}}}_{\text{bin effect}} \]

    \begin{itemize}
      \item Point Effect \(\Rightarrow\) evaluation \alert{on instances}
    \begin{itemize}
    \item Fast \( \rightarrow \) use of auto-differentiation, all derivatives in a single pass
    \item Versatile \( \rightarrow\) point effects computed once, change bins without cost
    \item Secure \( \rightarrow\) does not create artificial instances
    \end{itemize}
    \end{itemize}

  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  For \alert{differentiable} models, DALE resolves ALE weaknesses
\end{frame}

\begin{frame}
  \frametitle{DALE is faster}
  \begin{figure}
   \centering
   \includegraphics[width=.49\textwidth]{case-1-plot-1}
   \includegraphics[width=.49\textwidth]{case-1-plot-2}
   \caption{Light setup; small dataset \((N=10^2\) instances), light \(f\). Heavy setup; big dataset (\(N=10^5\) instances), heavy \(f\)}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  DALE accelerates the estimation
\end{frame}


\begin{frame}
  \frametitle{DALE may be more accurate - 40 Bins}
  \begin{figure}
    \centering
    \includegraphics<1>[width=0.6\textwidth]{bin_splitting_40_bins}
    \includegraphics<2>[width=.49\textwidth]{dale_40_bins}
    \includegraphics<2>[width=.49\textwidth]{ale_40_bins}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  \begin{itemize}
  \item DALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \item ALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DALE may be more accurate - 20 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{bin_splitting_20_bins}
    \includegraphics<2>[width=0.49\textwidth]{dale_20_bins}
    \includegraphics<2>[width=0.49\textwidth]{ale_20_bins}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  \begin{itemize}
  \item DALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \item ALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{DALE may be more accurate - 10 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{bin_splitting_10_bins}
    \includegraphics<2>[width=0.49\textwidth]{dale_10_bins}
    \includegraphics<2>[width=0.49\textwidth]{ale_10_bins}
    \label{}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  \begin{itemize}
  \item DALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \item ALE: starts being OOD, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DALE may be more accurate - 5 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{bin_splitting_5_bins}
    \includegraphics<2>[width=0.49\textwidth]{dale_5_bins}
    \includegraphics<2>[width=0.49\textwidth]{ale_5_bins}
    \label{}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  \begin{itemize}
  \item DALE: on-distribution, robust bin effect \(\rightarrow\) \textcolor{green}{good estimation}
  \item ALE: completely OOD, robust bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DALE may be more accurate - 3 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{bin_splitting_3_bins}
    \includegraphics<2>[width=0.49\textwidth]{dale_3_bins}
    \includegraphics<2>[width=0.49\textwidth]{ale_3_bins}
    \label{}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  \begin{itemize}
  \item DALE: on-distribution, robust bin effect \(\rightarrow\) \textcolor{green}{good estimation}
  \item ALE: completely OOD, robust bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}
